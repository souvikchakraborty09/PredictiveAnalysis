wbcd<-read.csv(file.choose(),header = T,stringsAsFactors = T)
str(wbcd)
wbcd<-wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis<-factor(wbcd$diagnosis, levels = c("B","M"),
labels = c("Benign","Malignant"))
round(prop.table(table(wbcd$diagnosis))*100,digits = 1)
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
normalize<-function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train<-wbcd_n[1:469, ]
wbcd_test<-wbcd_n[470:569, ]
wbcd_train_labels<-wbcd[1:469, 1]
wbcd_test_labels<-wbcd[470:569, 1]
install.packages("class")
wbcd_test_pred <- knn(train=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
wbcd_test_pred <- knn(train=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
library(class)
wbcd_test_pred <- knn(train=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
install.packages("gmodels")
library(gmodels)
CrossTable(x = wbcd_test_labels,y=wbcd_test_pred,
prop.chisq=FALSE)
aa<-table(wbcd_test_labels,wbcd_test_pred)
install.packages("caret")
library(caret)
confusionMatrix(aa)
View(wbcd)
table(wbcd$diagnosis)
CrossTable(x = wbcd_test_labels,y=wbcd_test_pred,
prop.chisq=FALSE)
aa<-table(wbcd_test_labels,wbcd_test_pred)
confusionMatrix(aa)
##load data
View(diamonds)
df<-data(diamonds)
str(diamonds)
summary(diamonds)
##See the structure
head(diamonds)
#Generate a random numbers sample that is 90% of the total no. of rows of dataset.
ran<-sample(1:nrow(diamonds), 0.9*nrow(diamonds))
str(diamonds)
range(diamonds$carat)
range(diamonds$price)
##the normalization function is created.
nor<-function(x){ (x-min(x))/(max(x)-min(x)) }
dia_norm<-as.data.frame(lapply(diamonds[,c(1,5,6,7,8,9,10)], nor))
summary(diamonds)
summary(dia_norm)
cw<-ChickWeight
View(cw)
str(cw)
head(cw)
##Generate a random sample that is 90% of the dataset.
ran <- sample(1:nrow(cw), 0.9*nrow(cw))
##Create a normalization function
nor<-function(x){ ((x-min(x))/(max(x)-min(x))) }
##Run normalization on 1st column  and 2nd column as they are predicators and column 3 and column 4 is a categorical column.
cw_norm <- as.data.frame(lapply(cw[,c(1,2)],nor))
##extracting_train_Set
cw_train <- cw_norm[ran,]
##extracting_test_Set
cw_test <- cw_norm[-ran,]
summary(iris)
summary(iris_norm)
##extract 5th column of train dataset because it iwll be used as 'cl' argument in
##function.
cw_target_category<-cw[ran,3]
View(iris)
##extract 5th column if test dataset to measure the accuracy
cw_test_category<-iris[-ran,4]
library(class)
##run knn function
pr<-knn( cw_train,cw_test,cl=cw_target_category,k=10)
##Create confusion matrix
tab <- table(pr,cw_test_category)
##run knn function
pr<-knn( cw_train,cw_test,cl=cw_target_category,k=1)
##Create confusion matrix
tab <- table(pr,cw_test_category)
##extract 5th column if test dataset to measure the accuracy
cw_test_category<-cw[-ran,4]
##run knn function
pr<-knn( cw_train,cw_test,cl=cw_target_category,k=24)
##Create confusion matrix
tab <- table(pr,cw_test_category)
##this function divides the correct predictions by total number of predictions
##that tells us how accurate the model is.
tab
accuracy<-function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
##extract 3th column of train dataset because it will be used as 'cl' argument in
##function.
cw_target_category<-cw[ran,c(3,4)]
##extract 5th column if test dataset to measure the accuracy
cw_test_category<-cw[-ran,c(3,4)]
##run knn function
pr<-knn( cw_train,cw_test,cl=cw_target_category,k=24)
#for all tweets
Twitter_data=read.csv(file.choose(),header=TRUE)
View(Twitter_data)
#for all tweets
Twitter_data=read.csv(file.choose(),header=TRUE,stringsAsFactors = T)
#for all tweets
Twitter_data=read.csv(file.choose(),header=TRUE)
#for all tweets
Twitter_data=read.csv(file.choose(),header=TRUE)
View(Twitter_data)
str(Twitter_data)
Twitter_data$text=factor(Twitter_data$text)
str(Twitter_data$text)
install.packages("tm")
library(tm)
corpus <- iconv(Twitter_data$text)
sms_corpus <- Corpus(VectorSource(Twitter_data$text))
print(sms_corpus)
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
sms_corpus_clean
sms_corpus_clean<-tm_map(sms_corpus_clean,removeNumbers)
sms_corpus_clean<-tm_map(sms_corpus_clean,
removeWords,stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
install.packages("SnowballC")
library(SnowballC)
sms_corpus_clean<-tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean<-tm_map(sms_corpus_clean, stripWhitespace)
as.character(sms_corpus_clean[[1]])
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm)
sms_dtm_train <- sms_dtm[1:4619]
inspect(sms_dtm_train)
sms_dtm_test <- sms_dtm[4170:5559]
inspect(sms_dtm_test)
sms_train_labels <- sms_raw[1:4169]$type
sms_train_labels <- sms_raw[1:4169]$type
install.packages("worldcloud")
library(worldcloud)
stm_dtm_freq_train <- sms_dtm_train[, sms_freq_words]
convert_counts <- function (x) {
x<- ifelse (x>0 , "Yes" , "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN=2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN=2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
install.packages("gmodels")
library(gmodel)
sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install.packages("tm")
library(tm)
# First step for text mining is to create corpus,
#which is the collection of text documents
# types of corpus - Vcorpus, Pcorpus
#VectorCorpus reader function which used to create source object
#from existing vector
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# First step for text mining is to create corpus,
#which is the collection of text documents
# types of corpus - Vcorpus, Pcorpus
#VectorCorpus reader function which used to create source object
#from existing vector
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
sms_corpus_clean
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean,
removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
View(sms_dtm)
#######word cloud chart######
#install.packages("wordcloud")
library("wordcloud")
#######word cloud chart######
install.packages("wordcloud")
install.packages("wordcloud")
library("wordcloud")
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F)
#install.packages("RColorBrewer")
library("RColorBrewer")
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F,
color=brewer.pal(8,"Dark2"),rot.per = 0.20)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
findFreqTerms(sms_dtm_train, 5)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
install.packages("wordcloud")
install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
??findFreqTerms
install.packages("tm")
install.packages("tm")
library(tm)
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
#setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes/Datasets")
data <- read.csv(file.choose(),stringsAsFactors = TRUE)
View(data)
str(data)
summary(data)
data <- subset(data, select = c("PassengerId","Survived","Sex","Age","Fare","Pclass"))
View(data)
summary(data)
data$Age[is.na(data$Age)]<-mean(data$Age,na.rm = TRUE)
data$Fare[is.na(data$Fare)]<- mean(data$Fare,na.rm = TRUE)
#data$Survived <- as.factor(data$Survived)
data$Survived <- cut(data$Survived,2,levels=c(1,0), labels = c('Yes','No'))
data$Pclass <- cut(data$Pclass,3,levels=c(1,2,3),labels = c('Low','Medium','High'))
set.seed(678)
ran <- sample(1:nrow(data), 0.90 * nrow(data))
nor <- function(x) { (x - min(x)) / (max(x) - min(x)) }
cols<- c('PassengerId','Age','Fare')
data_norm <- data %>% mutate_at(cols,nor)
#mutate(data$Pclass = factor(data$Pclass,levels = c(1,2,3),labels = c('Upper','Middle','Lower')))
install.packages("dplyr")
data_norm <- data %>% mutate_at(cols,nor)
library(dplyr)
data_norm <- data %>% mutate_at(cols,nor)
summary(data_norm)
train_data <-data_norm[ran,]
test_data <- data_norm[-ran,]
decision_tree_model = rpart(Survived~., data = train_data, method = "class")
library(rpart)
library(rpart.plot)
decision_tree_model = rpart(Survived~., data = train_data, method = "class")
plot(decision_tree_model)
rpart.plot(decision_tree_model)
data_predict = predict(decision_tree_model,test_data,type = "class")
data_prediction_table = table(test_data[,2],data_predict)
data_prediction_table
(data_performance = sum(diag(data_prediction_table))/sum(data_prediction_table))*100
data<-read.csv(file.choose(),stringsAsFactors = T)
data<-read.csv(file.choose(),stringsAsFactors = T)
View(data)
set.seed(678)
s=sample(nrow(data),100)
iris_train=iris[s, ]
iris_train=data[s, ]
iris_test=data[-s,]
iris_decision_tree_model = rpart(quality~., data=data_train, method = "class")
library(rpart)
library(rpart.plot)
data_decision_tree_model = rpart(quality~., data=data_train, method = "class")
data_train=data[s, ]
data_test=data[-s,]
data_decision_tree_model = rpart(quality~., data=data_train, method = "class")
data_decision_tree_model
plot(data_decision_tree_model)
text(data_decision_tree_model)
rpart.plot(data_decision_tree_model)
rpart.plot(data_decision_tree_model, type=4, extra=103)
data_predict = predict(data_decision_tree_model, data_test, type="class")
data_predict_table=table(data_test[,12], iris_predict)
data_predict_table=table(data_test[,12], data_predict)
data_predict_table
(data_performance=sum(diag(data_predict_table))/sum(data_predict_table))*100
library(rpart)
library(rpart.plot)
library(caret)
# Load the dataset
data <- read.csv(file.choose(),stringsAsFactors = T)
# Assuming 'quality' is the target variable
X <- data[, -ncol(data)]
y <- data$quality
# Split the data into training and testing sets
set.seed(42)
splitIndex <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- data[splitIndex, ]
test_data <- data[-splitIndex, ]
# Create a decision tree model
model <- rpart(quality ~ ., data = train_data)
# Make predictions on the test set
predictions <- predict(model, test_data, type = "class")
library(rpart)
library(rpart.plot)
# Create a decision tree model
model <- rpart(quality ~ ., data = train_data)
# Make predictions on the test set
predictions <- predict(model, test_data, type = "class")
library(rpart)
library(rpart.plot)
# Load the dataset
file_path <- file.choose()  # Alternatively, manually set the file path
wine_data <- read.csv(file_path, sep = ",", stringsAsFactors = TRUE)
# Split the data into training and testing sets (70:30 ratio)
set.seed(123)
sample_index <- sample(seq_len(nrow(wine_data)), size = 0.7 * nrow(wine_data))
train_data <- wine_data[sample_index, ]
test_data <- wine_data[-sample_index, ]
# Fit the decision tree model
model <- rpart(quality ~ ., data = train_data, method = "class")
# Make predictions on the test data
predictions <- predict(model, test_data, type = "class")
# Create confusion matrix
conf_matrix <- table(predictions, test_data$quality)
print("Confusion Matrix:")
print(conf_matrix)
# Visualize the decision tree
rpart.plot(model)
# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")
