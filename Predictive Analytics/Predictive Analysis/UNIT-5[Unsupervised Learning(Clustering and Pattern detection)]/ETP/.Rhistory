data<-read.csv(file.choose(),stringsAsFactors = T)
View(data)
summary(data)
str(aq)
str(data)
#Normalization
nor <- function(x){(x-min(x)) / (max(x)-min(x)) }
data_norm <-as.data.frame(lapply(data[c(13,19,20)], nor))
data[c(13,19,20)] <- data_norm
attach(data)
library(ggplot2)
#model
MULTIREG <- lm(Attrition~HourlyRate + MonthlyIncome + MonthlyRate)
data$Attrition=factor(data$Attrition,levels = c('Yes','No'),labels = c(0,1))
#model
MULTIREG <- lm(Attrition~HourlyRate + MonthlyIncome + MonthlyRate)
data$Attrition=factor(data$Attrition,levels = c('Yes','No'),labels = c(0,1))
data#Normalization
view(data)#Normalization
View(data)#Normalization
data$Attrition=factor(data$Attrition,levels = c('Yes','No'),labels = c(0,1),replace=TRUE)
data$Attrition=factor(data$Attrition,
levels = c('Yes','No'),
labels = c(0,1))
View(data)#Normalization
summary(data)
View(data)
library(ggplot2)
data<-read.csv(file.choose(),stringsAsFactors = T)
View(data)
str(data)
data$Attrition=factor(data$Attrition,
levels = c('Yes','No'),
labels = c(0,1))
View(data)#Normalization
nor <- function(x){(x-min(x)) / (max(x)-min(x)) }
data_norm <-as.data.frame(lapply(data[c(13,19,20)], nor))
data[c(13,19,20)] <- data_norm
attach(data)
#model
MULTIREG <- lm(Attrition~HourlyRate + MonthlyIncome + MonthlyRate)
summary(MULTIREG)
attributes(MULTIREG)
#test data
checkozone = data.frame(HourlyRate = 94, MonthlyIncome = 593, MonthlyRate =44497)
#test data
check = data.frame(HourlyRate = 94, MonthlyIncome = 593, MonthlyRate =44497)
#Predict
result = predict(MULTIREG, check)
result
##Visualization of model
plot( ~ MULTIREG$fit)
##Visualization of model
plot( ~ MULTIREG$Attrition)
df<-read.csv(file.choose(),stringsAsFactors = T)
View(df)
##Generate a random number that is 90% of the total number of rows in dataset.
ran<-sample(1:nrow(df), 0.9*nrow(df))
str(df)
range(df$HourlyRate)
range(df$MonthlyIncome)
range(df$MonthlyRate)
View(df)
##the normalization function is created.
nor<-function(x){ (x-min(x))/(max(x)-min(x)) }
df_norm<-as.data.frame(lapply(df[,c(13,19,20)], nor))
summary(df_norm)
summary(df)
##extracting training set
df_train <- df_norm[ran,]
##extracting testing set
df_test <- df_norm[-ran,]
##extract 5th column of train dataset because it iwll be used as 'cl' argument in
##function.
df_target_category<-df[ran,2]
##extract 5th column if test dataset to measure the accuracy
df_test_category<-df[-ran,2]
##load the package class
install.packages("class")
library(class)
##run knn function
pr<-knn(df_train,df_test,cl=df_target_category,k=10)
##Create confusion matrix
tab <- table(pr,df_test_category)
##this function divides the correct predictions by total number of predictions
##that tells us how accurate the model is.
tab
accuracy<-function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
View(df)
##Generate a random number that is 90% of the total number of rows in dataset.
ran<-sample(1:nrow(df), 0.9*nrow(df))
str(df)
range(df$HourlyRate)
range(df$MonthlyIncome)
range(df$MonthlyRate)
View(df)
#View(df)
##the normalization function is created.
nor<-function(x){ (x-min(x))/(max(x)-min(x)) }
df_norm<-as.data.frame(lapply(df[,c(13,19,20)], nor))
summary(df_norm)
summary(df)
##extracting training set
df_train <- df_norm[ran,]
##extracting testing set
df_test <- df_norm[-ran,]
df_target_category<-df[ran,2]
df_test_category<-df[-ran,2]
library(class)
##run knn function
pr<-knn(df_train,df_test,cl=df_target_category,k=10)
##Create confusion matrix
tab <- table(pr,df_test_category)
##this function divides the correct predictions by total number of predictions
##that tells us how accurate the model is.
tab
accuracy<-function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
##Generate a random number that is 90% of the total number of rows in dataset.
ran<-sample(1:nrow(df), 0.9*nrow(df))
str(df)
range(df$HourlyRate)
range(df$MonthlyIncome)
#View(df)
##the normalization function is created.
nor<-function(x){ (x-min(x))/(max(x)-min(x)) }
df_norm<-as.data.frame(lapply(df[,c(13,19,20)], nor))
summary(df_norm)
summary(df)
##extracting training set
df_train <- df_norm[ran,]
##extracting testing set
df_test <- df_norm[-ran,]
df_target_category<-df[ran,2]
df_test_category<-df[-ran,2]
#install.packages("class")
library(class)
##run knn function
pr<-knn(df_train,df_test,cl=df_target_category,k=10)
##Create confusion matrix
tab <- table(pr,df_test_category)
##this function divides the correct predictions by total number of predictions
##that tells us how accurate the model is.
tab
accuracy<-function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
View(df)
##Generate a random number that is 90% of the total number of rows in dataset.
ran<-sample(1:nrow(df), 0.9*nrow(df))
str(df)
range(df$HourlyRate)
range(df$MonthlyIncome)
#View(df)
##the normalization function is created.
nor<-function(x){ (x-min(x))/(max(x)-min(x)) }
df_norm<-as.data.frame(lapply(df[,c(13,19,20)], nor))
summary(df_norm)
summary(df)
##extracting training set
df_train <- df_norm[ran,]
##extracting testing set
df_test <- df_norm[-ran,]
df_target_category<-df[ran,2]
df_test_category<-df[-ran,2]
#install.packages("class")
library(class)
##run knn function
pr<-knn(df_train,df_test,cl=df_target_category,k=10)
##Create confusion matrix
tab <- table(pr,df_test_category)
##this function divides the correct predictions by total number of predictions
##that tells us how accurate the model is.
tab
accuracy<-function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
##this function divides the correct predictions by total number of predictions
##that tells us how accurate the model is.
tab
accuracy<-function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
library(caret)
# Load the Iris dataset
data(iris)
# Set the seed for reproducibility
set.seed(123)
# Create an index for splitting the data
index <- createDataPartition(iris$Species, p = 0.8, list = FALSE)
# Create training and testing datasets
training_data <- iris[index, ]
testing_data <- iris[-index, ]
# Normalize the data
normalize_data <- function(data) {
normalized_data <- scale(data[, 1:4])
colnames(normalized_data) <- colnames(data)[1:4]
return(cbind(normalized_data, Species = data$Species))
}
# Apply normalization to training and testing datasets
normalized_training_data <- normalize_data(training_data)
normalized_testing_data <- normalize_data(testing_data)
print(head(normalized_training_data))
print(head(normalized_testing_data))
from sklearn import datasets
# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
