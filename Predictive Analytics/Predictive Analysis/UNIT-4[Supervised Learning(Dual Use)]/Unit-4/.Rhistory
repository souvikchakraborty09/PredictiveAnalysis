concrete = read.csv(file.choose(),stringsAsFactors = T)
View(concrete)
str(concrete)
#total 9 variables are there. one is 'strength' dependent on all the
#other 8 variables.
#Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from zero
#up to over a thousand. So normalize the data
hist(concrete$strength)
summary(concrete_norm$strength)
normalize <- function(x) {  return((x - min(x)) / (max(x) - min(x)))}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
concrete = read.csv(file.choose(),stringsAsFactors = T)#Concrete.data
str(concrete)
#total 9 variables are there. one is 'strength' dependent on all the
#other 8 variables.
#Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from zero
#up to over a thousand. So normalize the data
hist(concrete$strength)
normalize <- function(x) {  return((x - min(x)) / (max(x) - min(x)))}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
library(neuralnet)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train)
plot(concrete_model)
model_results = compute(concrete_model,concrete_test[1:8])
plot(concrete_model)
#It returns a list with two components: $neurons, which stores the
#neurons for each layer in the network, and
#$net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water
+ superplasticizer + coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
plot(concrete_model2)
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
plot(concrete_model2)
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
# Importing the dataset
# Importing the dataset
#getwd()
#dataset = read.csv('social.csv')
dataset<- read.csv(file.choose(),sep=',',header = T)
str(dataset)
dataset = dataset[3:5]
View(dataset)
# Taking columns 3-5
#dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
View(dataset)
# Splitting the dataset into the Training set and Test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
View(training_set)
#scale is generic function whose default method centers and/or scales
#the columns of a numeric matrix.
test_set[-3] = scale(test_set[-3])
# Fitting SVM to the Training set
#install.packages('e1071')
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
# Plotting the training data set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
X1
grid_set = expand.grid(X1, X2)
#expand.grid will create dataframe
grid_set
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1),
length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1',
'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
#bg is background
#Visualizing the test set results
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
#if true, then add to the current plot
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1',
'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
data=iris
str(data)
summary(data)
library(caTools)
set.seed(123)
split = sample.split(data$Species, SplitRatio = 0.75)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
library(e1071)
classifier = svm(formula = Species ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-5])
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
cm = table(test_set[, 5], y_pred)
cm
1 - sum(diag(cm)) / sum(cm)
